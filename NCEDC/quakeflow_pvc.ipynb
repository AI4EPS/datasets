{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "numerous-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-shade",
   "metadata": {},
   "source": [
    "## Download Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indonesian-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_catalog(data_path=\"/tmp/\"):\n",
    "    \n",
    "    # %%\n",
    "    import urllib\n",
    "    import urllib.request as request\n",
    "    import re, os\n",
    "    from glob import glob\n",
    "    from tqdm import tqdm\n",
    "    import collections\n",
    "    import pickle\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # %%\n",
    "    root_dir = \"catalogs\"\n",
    "    root_url = \"http://ncedc.org/ftp/pub/catalogs/ncss/hypoinverse/phase2k\"\n",
    "    if not os.path.exists(root_dir):\n",
    "        os.mkdir(root_dir)\n",
    "\n",
    "    # %%\n",
    "    def get_years(root_url):\n",
    "        html = urllib.request.urlopen(root_url).read().decode()\n",
    "        pattern = re.compile(\"<a href=\\\"\\d\\d\\d\\d/\\\">\", re.S)\n",
    "        tmp_years = re.findall(pattern, html)\n",
    "        years = [re.findall(\"\\d\\d\\d\\d\", yr)[0] for yr in tmp_years]#[-1:]\n",
    "        year_urls = {yr: root_url+\"/\"+yr for yr in years}\n",
    "        return year_urls\n",
    "\n",
    "    year_urls = get_years(root_url)\n",
    "\n",
    "    # %%\n",
    "    def get_files(year_urls):\n",
    "        file_urls = {}\n",
    "        for year, url in year_urls.items():\n",
    "            html = urllib.request.urlopen(url).read().decode()\n",
    "            pattern = re.compile(\"<a href=\\\".*?\\.phase\\.Z\\\">\", re.S)\n",
    "            tmp_files = re.findall(pattern, html)\n",
    "            files = [re.findall(\"\\d.*?\\.phase\\.Z\", fl)[0] for fl in tmp_files]\n",
    "            file_urls[year] = [url+\"/\"+fl for fl in files]\n",
    "        return file_urls\n",
    "\n",
    "    file_urls = get_files(year_urls)\n",
    "    \n",
    "    # %%\n",
    "    def download_files(file_urls, root_dir):\n",
    "        for year in file_urls:\n",
    "            data_dir = os.path.join(root_dir, year)\n",
    "            if not os.path.exists(data_dir):\n",
    "                os.makedirs(data_dir)\n",
    "#             for url in tqdm(file_urls[year], desc=\"Downloading\"):\n",
    "            for url in file_urls[year]:\n",
    "                print(\"Downloading: \"+url)\n",
    "                request.urlretrieve(url, os.path.join(data_dir, url.split('/')[-1]))\n",
    "                os.system(\"uncompress \"+os.path.join(data_dir, url.split('/')[-1]))\n",
    "\n",
    "    download_files(file_urls, root_dir)\n",
    "\n",
    "    # %%\n",
    "    def merge_files(file_urls, root_dir, fout):\n",
    "\n",
    "        catlog = []\n",
    "        for year in file_urls:\n",
    "#             for url in tqdm(file_urls[year], desc=\"Merging\"):\n",
    "            for url in file_urls[year]:\n",
    "                print(f\"Merging: {url}\")\n",
    "                with open(os.path.join(root_dir, year, url.split('/')[-1].rstrip(\".Z\")), 'r') as fp:\n",
    "                    lines = fp.readlines()\n",
    "                    catlog += lines\n",
    "\n",
    "        with open(fout, 'w') as fp:\n",
    "#             for line in tqdm(catlog, desc=\"Writing catalog\"):\n",
    "            for line in catlog:\n",
    "                fp.write(line)\n",
    "        print(f\"Finish writing {len(catlog)} lines to {fout}\")\n",
    "\n",
    "    merge_files(file_urls, root_dir, \"catalogs.txt\")\n",
    "\n",
    "    # %%\n",
    "    def build_dict(catalog):\n",
    "        dataset1 = collections.OrderedDict()\n",
    "        with open(catalog) as fp:\n",
    "            for line in fp:\n",
    "                if line[0].isspace():\n",
    "                    continue\n",
    "                elif len(line) > 130:\n",
    "                    event_id = line\n",
    "                    dataset1[event_id] = []\n",
    "                elif len(line) <= 130:\n",
    "                    dataset1[event_id].append(line)\n",
    "                else:\n",
    "                    print(\"Unrecognized line: %s\" % line)\n",
    "\n",
    "        # dataset organized by event then station\n",
    "        dataset2 = collections.OrderedDict()\n",
    "#         for event in tqdm(dataset1, desc=\"Build dict\"):\n",
    "        for event in dataset1:\n",
    "            print(f\"Build dict: {event[:16]}\")\n",
    "            stationset = collections.OrderedDict()\n",
    "            for line in dataset1[event]:\n",
    "                # if line[111:113] not in [\"  \", \"--\", \"00\"]:\n",
    "                #     sta_id = line[:7]+line[111:113]#plus location code\n",
    "                # else:\n",
    "                #     sta_id = line[:7]+\"--\"\n",
    "                sta_id = line[:7]+line[111:113]\n",
    "                if sta_id in stationset:\n",
    "                    stationset[sta_id].append(line)\n",
    "                else:\n",
    "                    stationset[sta_id] = [line]\n",
    "            dataset2[event] = stationset\n",
    "\n",
    "        return dataset2\n",
    "\n",
    "    catalog_dict = build_dict(\"catalogs.txt\")\n",
    "    \n",
    "    # %%\n",
    "    def extract_ps(catalog):\n",
    "\n",
    "        # pick the best P and S pickers\n",
    "        dataset = collections.OrderedDict()\n",
    "#         for event in tqdm(catalog, desc=\"Extract P/S picks\"):\n",
    "        for event in catalog:\n",
    "            print(f\"Extract P/S picks: {event[:16]}\")\n",
    "            stationset = collections.OrderedDict()\n",
    "            for sta_id in catalog[event]:\n",
    "                best_p = 10\n",
    "                best_s = 10\n",
    "                id_p = 0\n",
    "                id_s = 0\n",
    "                found_p = 0\n",
    "                found_s = 0\n",
    "                for j, line in enumerate(catalog[event][sta_id]):\n",
    "                    if line[14] == 'P':\n",
    "                        if int(line[16]) < best_p:\n",
    "                            best_p = int(line[16])\n",
    "                            id_p = j\n",
    "                            found_p = 1\n",
    "                    if line[47] == 'S':\n",
    "                        if int(line[49]) < best_s:\n",
    "                            best_s = int(line[20])\n",
    "                            id_s = j\n",
    "                            found_s = 1\n",
    "\n",
    "                if found_p and found_s:\n",
    "                    stationset[sta_id] = [catalog[event][sta_id][id_p], catalog[event][sta_id][id_s]]\n",
    "\n",
    "                dataset[event] = stationset\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    dataset_ps = extract_ps(catalog_dict)\n",
    "\n",
    "    # %%\n",
    "    def write_ps(dataset, fname, with_event=False):\n",
    "        with open(fname, 'w') as fp:\n",
    "#             for event in tqdm(dataset, desc=\"Write P/S picks\"):\n",
    "            for event in dataset:\n",
    "                print(f\"Write P/S picks: {event[:16]}\")\n",
    "                if with_event and len(dataset[event]) > 0:\n",
    "                    fp.write(event)\n",
    "                for sta_id in dataset[event]:\n",
    "                    for line in dataset[event][sta_id]:\n",
    "                        fp.write(line)\n",
    "\n",
    "                        \n",
    "    write_ps(dataset_ps, os.path.join(data_path, \"catalogs_ps.txt\"), with_event=True)\n",
    "    \n",
    "\n",
    "    # %%\n",
    "#     def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "\n",
    "#         storage_client = storage.Client()\n",
    "#         bucket = storage_client.bucket(bucket_name)\n",
    "#         blobs = storage_client.list_blobs(bucket_name)\n",
    "#         for blob in blobs:\n",
    "#             print(blob.name)\n",
    "\n",
    "#         blob = bucket.blob(destination_blob_name)\n",
    "#         blob.upload_from_filename(source_file_name, timeout=3600)\n",
    "#         print(f\"File {source_file_name} uploaded to {destination_blob_name}\")\n",
    "    \n",
    "#     upload_blob(\"ncedc\", \"catalogs_ps.txt\", \"catalogs/catalogs_ps.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "delayed-spell",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download_catalog()\n",
    "\n",
    "download_catalog_op = comp.func_to_container_op(download_catalog, \n",
    "                                      base_image='python:3.8',\n",
    "                                      packages_to_install= [\n",
    "                                          \"tqdm\",\n",
    "                                          \"google-cloud-storage\"\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faced-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def read_ps_catalog(data_path=\"/tmp/\") -> list:\n",
    "    \n",
    "    join_path = lambda x: os.path.join(data_path, x)\n",
    "    \n",
    "    import os\n",
    "    import pickle\n",
    "    from tqdm import tqdm\n",
    "    from collections import OrderedDict\n",
    "    \n",
    "    events = OrderedDict()\n",
    "    phases = OrderedDict()\n",
    "    index = -1\n",
    "    with open(join_path(\"catalogs_ps.txt\")) as fp:\n",
    "#         for line in tqdm(fp, desc=\"Read catalog\"):\n",
    "        for line in fp:\n",
    "            if len(line) > 130:\n",
    "                index += 1\n",
    "                event_line = line\n",
    "                events[index] = event_line\n",
    "                phases[index] = []\n",
    "            elif len(line) <= 130:\n",
    "                phase_line = line\n",
    "                phases[index].append(phase_line)\n",
    "            else:\n",
    "                print(\"Unrecognized line: %s\" % line)\n",
    "                \n",
    "    with open(join_path(\"events.pkl\"), \"wb\") as fp:\n",
    "        pickle.dump(events, fp)\n",
    "        \n",
    "    with open(join_path(\"phases.pkl\"), \"wb\") as fp:\n",
    "        pickle.dump(phases, fp)\n",
    "    \n",
    "#     return list(range(index))\n",
    "    num_parallel = 21\n",
    "    idxs = [[] for i in range(num_parallel)]\n",
    "    for i in range(index):\n",
    "#     for i in range(min(3, index)):\n",
    "        idxs[i - i//num_parallel*num_parallel].append(i)\n",
    "    \n",
    "    with open(join_path(\"index.pkl\"), \"wb\") as fp:\n",
    "        pickle.dump(idxs, fp)\n",
    "        \n",
    "    print(f\"Events number: {index}\")\n",
    "    print(f\"Parallel number: {num_parallel}\")\n",
    "\n",
    "    return list(range(num_parallel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "emerging-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_ps_catalog()\n",
    "\n",
    "read_ps_catalog_op = comp.func_to_container_op(read_ps_catalog, \n",
    "                                              base_image='python:3.8',\n",
    "                                              packages_to_install= [\n",
    "                                                  \"tqdm\",\n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "silent-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(i: int, data_path=\"/tmp/\"):\n",
    "\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from collections import namedtuple, OrderedDict\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "    client = Client(\"NCEDC\")\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    join_path = lambda x: os.path.join(data_path, x)\n",
    "    with open(join_path(\"index.pkl\"), \"rb\") as fp:\n",
    "        index = pickle.load(fp)[i]\n",
    "    if len(index) == 0:\n",
    "        print(\"len(index) = 0\")\n",
    "        return\n",
    "    \n",
    "    # %%\n",
    "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(source_file_name, timeout=3600)\n",
    "        print(f\"File {source_file_name} uploaded to {destination_blob_name}\")\n",
    "    \n",
    "    # %%\n",
    "    def to_float(string):\n",
    "        if string.strip() == '':\n",
    "            return 0\n",
    "        else:\n",
    "            return float(string.strip())\n",
    "\n",
    "    Event = namedtuple(\"event\", [\"time\", \"latitude\", \"longitude\",\n",
    "                                 \"depth_km\", \"magnitude\", \"magnitude_type\", \"index\"])\n",
    "    def read_event_line(line, idx):\n",
    "        time = line[:4]+\"-\"+line[4:6]+\"-\"+line[6:8]+\"T\"+line[8:10]+\":\"+line[10:12]+\":\"+line[12:14]+\".\"+line[14:16] \n",
    "        latitude = to_float(line[16:18]) + to_float(line[19:23])/6000.0\n",
    "        longitude = -(to_float(line[23:26]) + to_float(line[27:31])/6000.0)\n",
    "        if line[18] == 'S':\n",
    "            latitude *= -1.0\n",
    "        if line[26] == 'E':\n",
    "            longitude *= -1.0\n",
    "        depth_km = to_float(line[31:36])/100.0\n",
    "        magnitude_type = (line[146:147]).strip()\n",
    "        magnitude = to_float(line[147:150])/100.0\n",
    "        return Event(time=time, latitude=latitude, longitude=longitude, \n",
    "                     depth_km=depth_km, magnitude=magnitude, magnitude_type=magnitude_type, index=idx)\n",
    "\n",
    "    def read_p_pick(line):\n",
    "        if float(line[30:32].replace(\" \", \"0\")) < 60:\n",
    "            tp = (line[17:21]+\"-\"+line[21:23]+\"-\"\n",
    "                +line[23:25]+\"T\"+line[25:27]+\":\"\n",
    "                +line[27:29]+\":\"+line[30:32]+'.'\n",
    "                +line[32:34]).replace(\" \", \"0\")\n",
    "            tp = obspy.UTCDateTime(tp)\n",
    "        else:\n",
    "            tp = (line[17:21]+\"-\"+line[21:23]+\"-\"\n",
    "                +line[23:25]+\"T\"+line[25:27]+\":\"\n",
    "                +line[27:29]+\":\"+'00'+'.'\n",
    "                +line[32:34]).replace(\" \", \"0\")\n",
    "            tp = obspy.UTCDateTime(tp) + float(line[30:32].replace(\" \", \"0\"))\n",
    "        remark = line[13:15].strip()\n",
    "        weight = line[16:17].strip()\n",
    "        channel = line[9:12].strip()\n",
    "        return tp, remark, weight, channel\n",
    "\n",
    "    def read_s_pick(line):\n",
    "        if float(line[42:44].replace(\" \", \"0\")) < 60:\n",
    "            ts = (line[17:21]+\"-\"+line[21:23]+\"-\"\n",
    "                +line[23:25]+\"T\"+line[25:27]+\":\"\n",
    "                +line[27:29]+\":\"+line[42:44]+'.'\n",
    "                +line[44:46]).replace(\" \", \"0\")\n",
    "            ts = obspy.UTCDateTime(ts)\n",
    "        else:\n",
    "            ts = (line[17:21]+\"-\"+line[21:23]+\"-\"\n",
    "                +line[23:25]+\"T\"+line[25:27]+\":\"\n",
    "                +line[27:29]+\":\"+\"00\"+'.'\n",
    "                +line[44:46]).replace(\" \", \"0\")\n",
    "            ts = obspy.UTCDateTime(ts) + float(line[42:44].replace(\" \", \"0\"))\n",
    "        remark = line[46:48].strip()\n",
    "        weight = line[49:50].strip()\n",
    "        channel = line[9:12].strip()\n",
    "        return ts, remark, weight, channel\n",
    "\n",
    "    Pick = namedtuple(\"pick\",[\"p_time\", \"p_remark\", \"p_weight\", \"p_channel\",\n",
    "                              \"s_time\", \"s_remark\", \"s_weight\", \"s_channel\",\n",
    "                              \"first_motion\", \"distance_km\",\n",
    "                              \"emergence_angle\", \"azimuth\", \n",
    "                              \"network\", \"station\", \"location_code\", \"event_index\"])\n",
    "    def read_phase_line(p_line, s_line, index):\n",
    "\n",
    "        line = p_line\n",
    "        network = (line[5:7]).strip()\n",
    "        station = (line[:5]).strip()\n",
    "        location_code = (line[111:113]).strip()\n",
    "        distance_km = to_float(line[74:78])/10.0\n",
    "        emergence_angle = to_float(line[78:81])\n",
    "        # duration = to_float(line[87:91])\n",
    "        azimuth = to_float(line[91:94])\n",
    "        first_motion = (line[15:16]).strip()\n",
    "        p_time, p_remark, p_weight, p_channel = read_p_pick(p_line)\n",
    "        s_time, s_remark, s_weight, s_channel = read_s_pick(s_line)\n",
    "\n",
    "        return Pick(p_time=p_time, p_remark=p_remark, p_weight=p_weight, p_channel=p_channel,\n",
    "                    s_time=s_time, s_remark=s_remark, s_weight=s_weight, s_channel=s_channel,\n",
    "                    first_motion=first_motion, distance_km=distance_km, \n",
    "                    emergence_angle=emergence_angle, azimuth=azimuth, \n",
    "                    network=network, station=station, location_code=location_code, event_index=index)\n",
    "\n",
    "    Station = namedtuple(\"station\", [\"id\", \"latitude\", \"longitude\", \"elevation_m\", \"unit\"])\n",
    "    def download_waveform(pick, waveform_path):\n",
    "\n",
    "        Tstart =  pick.p_time - 60.0\n",
    "        Tend = pick.p_time + 60.0\n",
    "\n",
    "        if pick.p_channel[:-1] != pick.s_channel[:-1]:\n",
    "            channels = [pick.p_channel[:-1], pick.s_channel[:-1]]\n",
    "        else: \n",
    "            channels = [pick.p_channel[:-1]]\n",
    "        channels = set(channels + [\"HN\", \"HH\", \"EH\", \"BH\", \"DP\"])\n",
    "\n",
    "        stream_list = []\n",
    "        station_list = []\n",
    "        for channel in channels:\n",
    "            fname = pick.network+\".\"+pick.station+\".\"+pick.location_code+\".\"+channel+\".\"+f\"{pick.event_index:07d}\"+\".mseed\"\n",
    "            # if os.path.exists(os.path.join(waveform_path, fname)):\n",
    "            #     stream = obspy.read(os.path.join(waveform_path, fname))\n",
    "            #     stream_list.append(stream)\n",
    "            # else:\n",
    "            try:\n",
    "                stream = client.get_waveforms(network=pick.network, station=pick.station, location=pick.location_code, \n",
    "                                            channel=channel+\"?\", starttime=Tstart, endtime=Tend)\n",
    "                stream = stream.detrend('linear')\n",
    "                stream = stream.merge(fill_value=0)\n",
    "                stream = stream.trim(Tstart, Tend, pad=True, fill_value=0)\n",
    "                stream = stream.sort()\n",
    "\n",
    "                station = client.get_stations(network=pick.network, station=pick.station, location=pick.location_code, \n",
    "                                            channel=channel+\"?\", starttime=Tstart, endtime=Tend, level=\"response\")\n",
    "                stream.attach_response(station)\n",
    "                stream.remove_sensitivity()\n",
    "                coord = station.get_coordinates(stream[0].get_id(), datetime=Tstart)\n",
    "                response = station.get_response(stream[0].get_id(), datetime=Tstart)\n",
    "                sta_id = pick.network+\".\"+pick.station+\".\"+pick.location_code+\".\"+channel\n",
    "                unit = response.instrument_sensitivity.input_units.lower()\n",
    "                station = Station(id=sta_id,\n",
    "                                    latitude=coord[\"latitude\"], longitude=coord[\"longitude\"], \n",
    "                                    elevation_m=coord[\"elevation\"], unit=unit)\n",
    "\n",
    "                station_list.append(station)\n",
    "                stream_list.append(stream)\n",
    "#                 stream.write(os.path.join(waveform_path, fname),  format=\"MSEED\")\n",
    "\n",
    "            except Exception as e:\n",
    "                if str(e)[:len(\"No data available\")] != \"No data available\":\n",
    "                    print(\"Failed downloading: \"+fname, \"Error: \"+str(e))\n",
    "\n",
    "        return stream_list, station_list\n",
    "\n",
    "    def calc_snr(vec, anchor, dt):\n",
    "        npts = int(3/dt)\n",
    "        snr = (np.std(vec[anchor:anchor+npts, :], axis=0)+1e-6) / (np.std(vec[anchor-npts:anchor, :], axis=0)+1e-6)\n",
    "        return snr\n",
    "\n",
    "    def data2vec(i, data, vec, shift, window_size):\n",
    "        if shift >= 0:\n",
    "            if len(data)+shift <= window_size:\n",
    "                vec[shift:len(data)+shift, i] = data\n",
    "            else:\n",
    "                vec[shift:, i] = data[:window_size-shift]\n",
    "        else:\n",
    "            if len(data)+shift <= window_size:\n",
    "                vec[:len(data)+shift, i] = data[-shift:]\n",
    "            else:\n",
    "                vec[:, i] = data[-shift:window_size-shift]\n",
    "        return vec\n",
    "\n",
    "    Extra = namedtuple(\"pick_extra\", [\"p_idx\", \"s_idx\", \"channels\", \"snr\", \"station_index\", \"latitude\", \"longitude\", \"elevation_m\", \"unit\", \"fname\"])\n",
    "    def convert_sample(pick, stream, station, sample_path):\n",
    "\n",
    "        dt = stream[-1].stats.delta\n",
    "        npts = stream[-1].stats.npts \n",
    "        starttime = stream[-1].stats.starttime                                  \n",
    "        endtime = stream[-1].stats.endtime\n",
    "\n",
    "        p_idx = int(np.around( (pick.p_time - starttime)/(endtime - starttime)*npts )) \n",
    "        s_idx = int(np.around( (pick.s_time - starttime)/(endtime - starttime)*npts ))\n",
    "\n",
    "        anchor = 6000\n",
    "        window_size = 12000\n",
    "        vec = np.zeros([window_size, 3])\n",
    "        shift = anchor - p_idx\n",
    "        if np.abs(shift) <= 1:\n",
    "            shift = 0\n",
    "        p_idx += shift\n",
    "        s_idx += shift\n",
    "\n",
    "        order = ['3','2','1','E','N','Z']\n",
    "        order = {key: i for i, key in enumerate(order)}\n",
    "        comps = [x.get_id() for x in stream]\n",
    "        comps = sorted(comps, key=lambda x: order[x[-1]])\n",
    "\n",
    "        fname = comps[0][:-1]+\".\"+f\"{pick.event_index:07d}\"+\".npz\"\n",
    "        if len(comps) == 3:\n",
    "            for i, c in enumerate(comps):\n",
    "                data = stream.select(id=c)[0].data\n",
    "                data2vec(i, data, vec, shift, window_size)\n",
    "        elif len(comps) < 3:\n",
    "            for c in comps:\n",
    "                if c[-1] == \"E\":\n",
    "                    i = 0\n",
    "                elif c[-1] == \"N\":\n",
    "                    i = 1\n",
    "                elif c[-1] == \"Z\":\n",
    "                    i = 2\n",
    "                else:\n",
    "                    print(f\"Unknown channels: {comps}\")\n",
    "                    return (1, None)\n",
    "                data = stream.select(id=c)[0].data\n",
    "                data2vec(i, data, vec, shift, window_size)\n",
    "        else:\n",
    "            print(f\"Unknown channels: {comps}\")\n",
    "            return (1, None)\n",
    "\n",
    "        snr = calc_snr(vec, anchor, dt)\n",
    "        channels = \",\".join([x.split(\".\")[-1] for x in comps])\n",
    "        extra = Extra(p_idx=p_idx, s_idx=s_idx, snr=tuple(snr.tolist()),\n",
    "                      station_index=station.id, channels=channels,\n",
    "                      latitude=station.latitude, longitude=station.longitude, elevation_m=station.elevation_m,\n",
    "                      unit=station.unit, fname=fname)\n",
    "        np.savez(os.path.join(sample_path, fname), \n",
    "                data=vec.astype(\"float32\"), dt=dt, p_idx=p_idx, s_idx=s_idx, snr=snr.tolist(), \n",
    "                tp=pick.p_time.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3], ts=pick.s_time.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3],\n",
    "                p_remark=pick.p_remark, p_weight=pick.p_weight, s_remark=pick.s_remark, s_weight=pick.s_weight,\n",
    "                first_motion=pick.first_motion, distance_km=pick.distance_km, \n",
    "                azimuth=pick.azimuth, emergence_angle=pick.emergence_angle, \n",
    "                network=pick.network, station=pick.station, location_code=pick.location_code, \n",
    "                station_latitude=station.latitude, station_longitude=station.longitude, elevation_m=station.elevation_m,\n",
    "                unit=station.unit, channels=channels, event_index=pick.event_index)\n",
    "\n",
    "        upload_blob(\"ncedc\", os.path.join(sample_path, fname), os.path.join(sample_path, fname))\n",
    "        os.remove(os.path.join(sample_path, fname))\n",
    "        return (0, extra)\n",
    "\n",
    "    # %%\n",
    "    def create_dataset(idx, events, phases, waveform_path, sample_path):\n",
    "        events_ = []\n",
    "        phases_ = []\n",
    "        extras_ = []\n",
    "#         for i in tqdm(idx, desc=\"create dataset\"):\n",
    "        for i in idx:\n",
    "            print(f\"Create dataset {i}/{idx[-1]}\")\n",
    "            event = read_event_line(events[i], i)\n",
    "            # events_.append(event)\n",
    "            phase_lines = phases[i]\n",
    "            has_phase = False\n",
    "            for j in range(0, len(phase_lines), 2):\n",
    "                pick = read_phase_line(phase_lines[j], phase_lines[j+1], i)\n",
    "                # phases_.append(pick)\n",
    "                (waveforms, stations) = download_waveform(pick, waveform_path)\n",
    "                for w, s in zip(waveforms, stations):\n",
    "                    (status, extra) = convert_sample(pick, w, s, sample_path)\n",
    "                    if (status == 0):\n",
    "                        phases_.append(pick)\n",
    "                        extras_.append(extra)\n",
    "                        has_phase = True\n",
    "            if has_phase:\n",
    "                events_.append(event)\n",
    "\n",
    "        return events_, phases_, extras_\n",
    "\n",
    "    waveform_path = \"waveforms\"\n",
    "    sample_path = \"dataset\"\n",
    "    if not os.path.exists(waveform_path):\n",
    "        os.mkdir(waveform_path)\n",
    "    if not os.path.exists(sample_path):\n",
    "        os.mkdir(sample_path)\n",
    "    # events_tuple, phases_tuple, extra_tuple = create_dataset([len(events)-1], events, phases, waveform_path, sample_path)\n",
    "    with open(join_path(\"events.pkl\"), \"rb\") as fp:\n",
    "        events = pickle.load(fp)\n",
    "    with open(join_path(\"phases.pkl\"), \"rb\") as fp:\n",
    "        phases = pickle.load(fp)\n",
    "        \n",
    "    events_tuple, phases_tuple, extra_tuple = create_dataset(index, events, phases, waveform_path, sample_path)\n",
    "#     events_tuple, phases_tuple, extra_tuple = create_dataset(range(len(events)//6, len(events)), events, phases, waveform_path, sample_path)\n",
    "\n",
    "    if len(events_tuple) >= 1:\n",
    "        events_df = pd.DataFrame(data=events_tuple)\n",
    "        events_df[\"latitude\"] = events_df[\"latitude\"].apply(lambda x: f\"{x:.4f}\")\n",
    "        events_df[\"longitude\"] = events_df[\"longitude\"].apply(lambda x: f\"{x:.4f}\")\n",
    "        events_df = events_df.set_index(\"index\")\n",
    "        events_df.to_csv(join_path(f\"{index[0]:03d}_events.csv\"), sep=\"\\t\")\n",
    "\n",
    "    if len(phases_tuple) >= 1:\n",
    "        phases_df = pd.DataFrame(data=phases_tuple)\n",
    "        extra_df = pd.DataFrame(data=extra_tuple)\n",
    "        phases_df[\"fname\"] = extra_df[\"fname\"]\n",
    "        phases_df[\"p_idx\"] = extra_df[\"p_idx\"]\n",
    "        phases_df[\"s_idx\"] = extra_df[\"s_idx\"]\n",
    "        phases_df[\"station_latitude\"] = extra_df[\"latitude\"].apply(lambda x: f\"{x:.4f}\")\n",
    "        phases_df[\"station_longitude\"] = extra_df[\"longitude\"].apply(lambda x: f\"{x:.4f}\")\n",
    "        phases_df[\"station_elevation_m\"] = extra_df[\"elevation_m\"]\n",
    "        phases_df[\"channels\"] = extra_df[\"channels\"]\n",
    "        phases_df[\"unit\"] = extra_df[\"unit\"]\n",
    "        phases_df[\"snr\"] = extra_df[\"snr\"].apply(lambda x: \",\".join([f\"{i:.2f}\" for i in x]))\n",
    "        phases_df[\"p_time\"] = phases_df[\"p_time\"].apply(lambda x: x.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3])\n",
    "        phases_df[\"s_time\"] = phases_df[\"s_time\"].apply(lambda x: x.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3])\n",
    "        phases_df.to_csv(join_path(f\"{index[0]:03d}_phases.csv\"), sep=\"\\t\", index=False,\n",
    "            columns=[\"fname\", \"network\",\"station\",\"location_code\", \n",
    "            \"p_idx\", \"p_time\",\"p_remark\",\"p_weight\",\n",
    "            \"s_idx\", \"s_time\",\"s_remark\",\"s_weight\",\n",
    "            \"first_motion\",\"distance_km\",\"emergence_angle\",\"azimuth\",\n",
    "            \"station_latitude\", \"station_longitude\", \"station_elevation_m\", \"unit\",\n",
    "            \"event_index\", \"channels\", \"snr\"])\n",
    "    \n",
    "    # %%\n",
    "    if os.path.exists(join_path(f\"{index[0]:03d}_events.csv\")):\n",
    "        upload_blob(\"ncedc\", join_path(f\"{index[0]:03d}_events.csv\"), os.path.join(\"catalogs\", f\"{index[0]:03d}_events.csv\"))\n",
    "    if os.path.exists(join_path(f\"{index[0]:03d}_phases.csv\")):\n",
    "        upload_blob(\"ncedc\", join_path(f\"{index[0]:03d}_phases.csv\"), os.path.join(\"catalogs\", f\"{index[0]:03d}_phases.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "synthetic-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_dataset(0)\n",
    "\n",
    "build_dataset_op = comp.func_to_container_op(build_dataset, \n",
    "                                             base_image='python:3.8',\n",
    "                                             packages_to_install= [\n",
    "                                                  \"tqdm\",\n",
    "                                                  \"obspy\",\n",
    "                                                  \"pandas\",\n",
    "                                                  \"google-cloud-storage\"\n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "waiting-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_result(data_path=\"/tmp/\"):\n",
    "    \n",
    "    from glob import glob\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    \n",
    "    join_path = lambda x: os.path.join(data_path, x)\n",
    "    \n",
    "    # %%\n",
    "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(source_file_name, timeout=3600)\n",
    "        print(f\"File {source_file_name} uploaded to {destination_blob_name}\")\n",
    "    \n",
    "    files_events = glob(join_path(\"[0-9]*_events.csv\"))\n",
    "    if len(files_events) > 0:\n",
    "        combined_events = pd.concat([pd.read_csv(f, sep=\"\\t\", dtype=str) for f in files_events ]).sort_values(by=\"index\")\n",
    "        combined_events.to_csv(join_path(\"combined_events.csv\"), sep=\"\\t\", index=False)\n",
    "        upload_blob(\"ncedc\", join_path(f\"combined_events.csv\"), os.path.join(\"catalogs\", f\"combined_events.csv\"))\n",
    "    else:\n",
    "        print(\"No events.csv found!\")\n",
    "    \n",
    "    files_phases = glob(join_path(\"[0-9]*_phases.csv\"))\n",
    "    if len(files_phases) > 0:\n",
    "        combined_phases = pd.concat([pd.read_csv(f, sep=\"\\t\", dtype=str) for f in files_phases ]).sort_values(by=[\"event_index\", \"fname\"])\n",
    "        combined_phases.to_csv(join_path(\"combined_phases.csv\"), sep=\"\\t\", index=False)\n",
    "        upload_blob(\"ncedc\", join_path(f\"combined_phases.csv\"), os.path.join(\"catalogs\", f\"combined_phases.csv\"))\n",
    "    else:\n",
    "        print(\"No phases.csv found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "harmful-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_result(\"./tmp\")\n",
    "\n",
    "merge_result_op = comp.func_to_container_op(merge_result, \n",
    "                                            base_image='python:3.8',\n",
    "                                            packages_to_install= [\n",
    "                                                  \"pandas\",\n",
    "                                                  \"google-cloud-storage\"\n",
    "                                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "surface-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(name='QuakeFlow', description='')\n",
    "def dataset_pipeline():\n",
    "    \n",
    "    data_path = \"/tmp/\"\n",
    "    vop = dsl.VolumeOp(name=\"Create_volume\",\n",
    "                       resource_name=\"data-volume\", \n",
    "                       size=\"20Gi\", \n",
    "                       modes=dsl.VOLUME_MODE_RWO)\n",
    "\n",
    "    raw_catalog = download_catalog_op(data_path).add_pvolumes({data_path: vop.volume})#.set_memory_request(\"10G\")\n",
    "    raw_catalog.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    ps_catalog = read_ps_catalog_op(data_path).add_pvolumes({data_path: raw_catalog.pvolume})\n",
    "    ps_catalog.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    with kfp.dsl.ParallelFor(ps_catalog.outputs[\"output\"]) as idx:\n",
    "        \n",
    "        build_dataset_op_ = build_dataset_op(idx, data_path).add_pvolumes({data_path: ps_catalog.pvolume})\n",
    "        build_dataset_op_.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "    result = merge_result_op(data_path).add_pvolumes({data_path: build_dataset_op_.pvolume})\n",
    "    result.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "retained-perth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://5d54cd3d08bfc52f-dot-us-west1.pipelines.googleusercontent.com/#/experiments/details/1b808809-57a1-4d1f-ac34-1dd8dc9334bd\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://5d54cd3d08bfc52f-dot-us-west1.pipelines.googleusercontent.com/#/runs/details/34fa38c8-ccbb-42ea-8f9f-d6ded5babd07\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/weiqiang/.dotbot/cloud/quakeflow.json\"\n",
    "\n",
    "host_url = '5d54cd3d08bfc52f-dot-us-west1.pipelines.googleusercontent.com'\n",
    "# host_url = \"https://eca4f7e37618.ngrok.io\"\n",
    "client = kfp.Client(host=host_url)\n",
    "\n",
    "experiment_name = 'Dataset'\n",
    "pipeline_func = dataset_pipeline\n",
    "run_name = pipeline_func.__name__ + '_run'\n",
    "\n",
    "arguments = {}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "# kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "results = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                               experiment_name=experiment_name, \n",
    "                                               run_name=run_name, \n",
    "                                               arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-plasma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-synthesis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
