{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "administrative-strike",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:09.746259Z",
     "iopub.status.busy": "2021-05-01T05:32:09.745917Z",
     "iopub.status.idle": "2021-05-01T05:32:10.720450Z",
     "shell.execute_reply": "2021-05-01T05:32:10.719603Z",
     "shell.execute_reply.started": "2021-05-01T05:32:09.746217Z"
    }
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-juvenile",
   "metadata": {},
   "source": [
    "## Download Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "understood-contest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:10.726165Z",
     "iopub.status.busy": "2021-05-01T05:32:10.725886Z",
     "iopub.status.idle": "2021-05-01T05:32:10.818984Z",
     "shell.execute_reply": "2021-05-01T05:32:10.818451Z",
     "shell.execute_reply.started": "2021-05-01T05:32:10.726128Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_catalog(catalog_path: OutputPath(str)):\n",
    "    \n",
    "    # %%\n",
    "    import urllib\n",
    "    import urllib.request as request\n",
    "    import re, os\n",
    "    from glob import glob\n",
    "#     from tqdm import tqdm\n",
    "    import collections\n",
    "    import pickle\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(\"quakeflow\")\n",
    "\n",
    "    def upload_blob(source_file_name, destination_blob_name):\n",
    "#         storage_client = storage.Client()\n",
    "#         bucket = storage_client.bucket(bucket_name)\n",
    "        while True:\n",
    "            try:\n",
    "                blob = bucket.blob(destination_blob_name)\n",
    "                blob.upload_from_filename(source_file_name, timeout=3600)\n",
    "                print(f\"File {source_file_name} uploaded to {destination_blob_name}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: File {source_file_name} uploade failed\\n{e}\")\n",
    "            \n",
    "    def download_blob(source_file_name, destination_blob_name):\n",
    "#         storage_client = storage.Client()\n",
    "#         bucket = storage_client.bucket(bucket_name)\n",
    "        while True:\n",
    "            try:\n",
    "                blob = bucket.blob(source_file_name)\n",
    "                blob.download_to_filename(destination_blob_name, timeout=3600)\n",
    "                print(f\"File {source_file_name} download to {destination_blob_name}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: File {source_file_name} download failed\\n{e}\")\n",
    "        \n",
    "    def exist_blob(source_file_name):\n",
    "#         storage_client = storage.Client()\n",
    "#         bucket = storage_client.bucket(bucket_name)\n",
    "        stats = storage.Blob(bucket=bucket, name=source_file_name).exists(storage_client)\n",
    "        if stats:\n",
    "            print(f\"File {source_file_name} exist\")\n",
    "        return stats\n",
    "    \n",
    "    # %%\n",
    "    root_dir = \"catalogs\"\n",
    "    root_url = \"http://ncedc.org/ftp/pub/catalogs/ncss/hypoinverse/phase2k\"\n",
    "    if not os.path.exists(root_dir):\n",
    "        os.mkdir(root_dir)\n",
    "        \n",
    "    catalog_exit = False\n",
    "    if exist_blob(\"ncedc_catalogs.txt\"):\n",
    "        download_blob(\"ncedc_catalogs.txt\", \"catalogs.txt\")\n",
    "        catalog_exit = True\n",
    "\n",
    "    # %%\n",
    "    def get_years(root_url):\n",
    "        html = urllib.request.urlopen(root_url).read().decode()\n",
    "        pattern = re.compile(\"<a href=\\\"\\d\\d\\d\\d/\\\">\", re.S)\n",
    "        tmp_years = re.findall(pattern, html)\n",
    "        years = [re.findall(\"\\d\\d\\d\\d\", yr)[0] for yr in tmp_years][:-1]\n",
    "        year_urls = {yr: root_url+\"/\"+yr for yr in years}\n",
    "        return year_urls\n",
    "\n",
    "    if not catalog_exit:\n",
    "        year_urls = get_years(root_url)\n",
    "\n",
    "    # %%\n",
    "    def get_files(year_urls):\n",
    "        file_urls = {}\n",
    "        for year, url in year_urls.items():\n",
    "            html = urllib.request.urlopen(url).read().decode()\n",
    "            pattern = re.compile(\"<a href=\\\".*?\\.phase\\.Z\\\">\", re.S)\n",
    "            tmp_files = re.findall(pattern, html)\n",
    "            files = [re.findall(\"\\d.*?\\.phase\\.Z\", fl)[0] for fl in tmp_files]\n",
    "            file_urls[year] = [url+\"/\"+fl for fl in files]\n",
    "        return file_urls\n",
    "\n",
    "    if not catalog_exit:\n",
    "        file_urls = get_files(year_urls)\n",
    "    \n",
    "    # %%\n",
    "    def download_files(file_urls, root_dir):\n",
    "        for year in file_urls:\n",
    "            data_dir = os.path.join(root_dir, year)\n",
    "            if not os.path.exists(data_dir):\n",
    "                os.makedirs(data_dir)\n",
    "#             for url in tqdm(file_urls[year], desc=\"Downloading\"):\n",
    "            for url in file_urls[year]:\n",
    "                print(\"Downloading: \"+url)\n",
    "                request.urlretrieve(url, os.path.join(data_dir, url.split('/')[-1]))\n",
    "                os.system(\"uncompress \"+os.path.join(data_dir, url.split('/')[-1]))\n",
    "\n",
    "    if not catalog_exit:\n",
    "        download_files(file_urls, root_dir)\n",
    "\n",
    "    # %%\n",
    "    def merge_files(file_urls, root_dir, fout):\n",
    "\n",
    "        catlog = []\n",
    "        for year in file_urls:\n",
    "#             for url in tqdm(file_urls[year], desc=\"Merging\"):\n",
    "            for url in file_urls[year]:\n",
    "#                 print(f\"Merging: {url}\")\n",
    "                with open(os.path.join(root_dir, year, url.split('/')[-1].rstrip(\".Z\")), 'r') as fp:\n",
    "                    lines = fp.readlines()\n",
    "                    catlog += lines\n",
    "\n",
    "        with open(fout, 'w') as fp:\n",
    "#             for line in tqdm(catlog, desc=\"Writing catalog\"):\n",
    "            for line in catlog:\n",
    "                fp.write(line)\n",
    "        print(f\"Finish writing {len(catlog)} lines to {fout}\")\n",
    "    \n",
    "    if not catalog_exit:\n",
    "        merge_files(file_urls, root_dir, \"catalogs.txt\")\n",
    "        upload_blob(\"catalogs.txt\", \"ncedc_catalogs.txt\")\n",
    "\n",
    "    # %%\n",
    "    def build_dict(catalog):\n",
    "        dataset1 = collections.OrderedDict()\n",
    "        with open(catalog) as fp:\n",
    "            for line in fp:\n",
    "                if line[0].isspace():\n",
    "                    continue\n",
    "                elif len(line) > 130:\n",
    "                    event_id = line\n",
    "                    dataset1[event_id] = []\n",
    "                elif len(line) <= 130:\n",
    "                    dataset1[event_id].append(line)\n",
    "                else:\n",
    "                    print(\"Unrecognized line: %s\" % line)\n",
    "\n",
    "        # dataset organized by event then station\n",
    "        dataset2 = collections.OrderedDict()\n",
    "#         for event in tqdm(dataset1, desc=\"Build dict\"):\n",
    "        print(\"Build dict:\")\n",
    "        for event in dataset1:\n",
    "#             print(f\"Build dict: {event[:16]}\")\n",
    "            stationset = collections.OrderedDict()\n",
    "            for line in dataset1[event]:\n",
    "                # if line[111:113] not in [\"  \", \"--\", \"00\"]:\n",
    "                #     sta_id = line[:7]+line[111:113]#plus location code\n",
    "                # else:\n",
    "                #     sta_id = line[:7]+\"--\"\n",
    "                sta_id = line[:7]+line[111:113]\n",
    "                if sta_id in stationset:\n",
    "                    stationset[sta_id].append(line)\n",
    "                else:\n",
    "                    stationset[sta_id] = [line]\n",
    "            dataset2[event] = stationset\n",
    "\n",
    "        return dataset2\n",
    "\n",
    "    catalog_dict = build_dict(\"catalogs.txt\")\n",
    "    \n",
    "    # %%\n",
    "    def extract_ps(catalog):\n",
    "\n",
    "        # pick the best P and S pickers\n",
    "        dataset = collections.OrderedDict()\n",
    "#         for event in tqdm(catalog, desc=\"Extract P/S picks\"):\n",
    "        print(\"Extract P/S picks:\")\n",
    "        for event in catalog:\n",
    "#             print(f\"Extract P/S picks: {event[:16]}\")\n",
    "            stationset = collections.OrderedDict()\n",
    "            for sta_id in catalog[event]:\n",
    "                best_p = 10\n",
    "                best_s = 10\n",
    "                id_p = 0\n",
    "                id_s = 0\n",
    "                found_p = 0\n",
    "                found_s = 0\n",
    "                for j, line in enumerate(catalog[event][sta_id]):\n",
    "                    if line[14] == 'P':\n",
    "                        if int(line[16]) < best_p:\n",
    "                            best_p = int(line[16])\n",
    "                            id_p = j\n",
    "                            found_p = 1\n",
    "                    if line[47] == 'S':\n",
    "                        if int(line[49]) < best_s:\n",
    "                            best_s = int(line[20])\n",
    "                            id_s = j\n",
    "                            found_s = 1\n",
    "\n",
    "                if found_p and found_s:\n",
    "                    stationset[sta_id] = [catalog[event][sta_id][id_p], catalog[event][sta_id][id_s]]\n",
    "\n",
    "                dataset[event] = stationset\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    dataset_ps = extract_ps(catalog_dict)\n",
    "\n",
    "    # %%\n",
    "    def write_ps(dataset, fname, with_event=False):\n",
    "        with open(fname, 'w') as fp:\n",
    "#             for event in tqdm(dataset, desc=\"Write P/S picks\"):\n",
    "            print(\"Write P/S picks:\")\n",
    "            for event in dataset:\n",
    "#                 print(f\"Write P/S picks: {event[:16]}\")\n",
    "                if with_event and len(dataset[event]) > 0:\n",
    "                    fp.write(event)\n",
    "                for sta_id in dataset[event]:\n",
    "                    for line in dataset[event][sta_id]:\n",
    "                        fp.write(line)\n",
    "\n",
    "    write_ps(dataset_ps, \"catalogs_ps.txt\", with_event=True)\n",
    "    \n",
    "\n",
    "    # %%\n",
    "#     def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "\n",
    "#         storage_client = storage.Client()\n",
    "#         bucket = storage_client.bucket(bucket_name)\n",
    "#         blobs = storage_client.list_blobs(bucket_name)\n",
    "#         for blob in blobs:\n",
    "#             print(blob.name)\n",
    "\n",
    "#         blob = bucket.blob(destination_blob_name)\n",
    "#         blob.upload_from_filename(source_file_name, timeout=3600)\n",
    "#         print(f\"File {source_file_name} uploaded to {destination_blob_name}\")\n",
    "    \n",
    "#     upload_blob(\"ncedc\", \"catalogs_ps.txt\", \"catalogs/catalogs_ps.txt\")\n",
    "\n",
    "    # %% \n",
    "    with open(catalog_path, \"w\") as fout:\n",
    "        with open(\"catalogs_ps.txt\") as fin:\n",
    "            for line in fin:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reflected-switzerland",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:10.821107Z",
     "iopub.status.busy": "2021-05-01T05:32:10.820885Z",
     "iopub.status.idle": "2021-05-01T05:32:10.974627Z",
     "shell.execute_reply": "2021-05-01T05:32:10.974009Z",
     "shell.execute_reply.started": "2021-05-01T05:32:10.821084Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download_catalog(credentials=credentials, \"test.txt\")\n",
    "\n",
    "download_catalog_op = comp.func_to_container_op(download_catalog, \n",
    "                                      base_image='python:3.8',\n",
    "                                      packages_to_install= [\n",
    "#                                           \"tqdm\",\n",
    "                                          \"google-cloud-storage\"\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "thrown-guess",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:10.977632Z",
     "iopub.status.busy": "2021-05-01T05:32:10.976185Z",
     "iopub.status.idle": "2021-05-01T05:32:11.027218Z",
     "shell.execute_reply": "2021-05-01T05:32:11.026417Z",
     "shell.execute_reply.started": "2021-05-01T05:32:10.977583Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "def read_ps_catalog(catalog: InputPath(str),\n",
    "                    index_path: OutputPath(\"pickle\"),\n",
    "                    events_path: OutputPath(\"pickle\"),\n",
    "                    phases_path: OutputPath(\"pickle\")) -> list:\n",
    "    \n",
    "    import pickle\n",
    "#     from tqdm import tqdm\n",
    "    from collections import OrderedDict\n",
    "    \n",
    "    events = OrderedDict()\n",
    "    phases = OrderedDict()\n",
    "    index = -1\n",
    "    with open(catalog) as fp:\n",
    "#         for line in tqdm(fp, desc=\"Read catalog\"):\n",
    "        for line in fp:\n",
    "            if len(line) > 130:\n",
    "                index += 1\n",
    "                event_line = line\n",
    "                events[index] = event_line\n",
    "                phases[index] = []\n",
    "            elif len(line) <= 130:\n",
    "                phase_line = line\n",
    "                phases[index].append(phase_line)\n",
    "            else:\n",
    "                print(\"Unrecognized line: %s\" % line)\n",
    "                \n",
    "    with open(events_path, \"wb\") as fp:\n",
    "        pickle.dump(events, fp)\n",
    "        \n",
    "    with open(phases_path, \"wb\") as fp:\n",
    "        pickle.dump(phases, fp)\n",
    "    \n",
    "#     return list(range(index))\n",
    "    num_parallel = 48*4 - 3\n",
    "    idxs = [[] for i in range(num_parallel)]\n",
    "    for i in range(index):\n",
    "#     for i in range(index-45*10,index):\n",
    "        idxs[i - i//num_parallel*num_parallel].append(i)\n",
    "    \n",
    "    with open(index_path, \"wb\") as fp:\n",
    "        pickle.dump(idxs, fp)\n",
    "        \n",
    "    print(f\"Events number: {index}\")\n",
    "    print(f\"Parallel number: {num_parallel}\")\n",
    "\n",
    "    return list(range(num_parallel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "national-emerald",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:11.028773Z",
     "iopub.status.busy": "2021-05-01T05:32:11.028360Z",
     "iopub.status.idle": "2021-05-01T05:32:11.084927Z",
     "shell.execute_reply": "2021-05-01T05:32:11.084268Z",
     "shell.execute_reply.started": "2021-05-01T05:32:11.028749Z"
    }
   },
   "outputs": [],
   "source": [
    "# read_ps_catalog(\"catalogs_ps.txt\", \"events.pkl\", \"phases.pkl\")\n",
    "\n",
    "read_ps_catalog_op = comp.func_to_container_op(read_ps_catalog, \n",
    "                                              base_image='python:3.8',\n",
    "                                              packages_to_install= [\n",
    "#                                                   \"tqdm\",\n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "apparent-buffer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:11.087491Z",
     "iopub.status.busy": "2021-05-01T05:32:11.087088Z",
     "iopub.status.idle": "2021-05-01T05:32:11.207734Z",
     "shell.execute_reply": "2021-05-01T05:32:11.206859Z",
     "shell.execute_reply.started": "2021-05-01T05:32:11.087451Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_dataset(i: int,\n",
    "                  index_input: InputPath(\"pickle\"),\n",
    "                  events_input: InputPath(\"pickle\"),\n",
    "                  phases_input: InputPath(\"pickle\"),\n",
    "                  events_path: OutputPath(str),\n",
    "                  phases_path: OutputPath(str)):\n",
    "    \n",
    "\n",
    "    import numpy as np\n",
    "#     from tqdm import tqdm\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from collections import namedtuple, OrderedDict\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "    client = Client(\"NCEDC\")\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # %%\n",
    "    join_path = lambda x: os.path.join(data_path, x)\n",
    "    with open(index_input, \"rb\") as fp:\n",
    "        index = pickle.load(fp)[i][::-1]\n",
    "#         index = pickle.load(fp)[i]\n",
    "    \n",
    "    if len(index) == 0:\n",
    "        print(\"len(index) = 0\")\n",
    "        \n",
    "        with open(events_path, \"w\") as fout:\n",
    "            fout.write(\"\")\n",
    "        with open(phases_path, \"w\") as fout:\n",
    "            fout.write(\"\")\n",
    "        return\n",
    "    \n",
    "    # %%\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(\"quakeflow\")\n",
    "\n",
    "    def upload_blob(source_file_name, destination_blob_name):\n",
    "#         storage_client = storage.Client()\n",
    "#         bucket = storage_client.bucket(bucket_name)\n",
    "        while True:\n",
    "            try:\n",
    "                blob = bucket.blob(destination_blob_name)\n",
    "                blob.upload_from_filename(source_file_name, timeout=3600)\n",
    "                print(f\"File {source_file_name} uploaded to {destination_blob_name}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: File {source_file_name} uploade failed\\n{e}\")\n",
    "            \n",
    "    def download_blob(source_file_name, destination_blob_name):\n",
    "#         storage_client = storage.Client()\n",
    "#         bucket = storage_client.bucket(bucket_name)\n",
    "        while True:\n",
    "            try:\n",
    "                blob = bucket.blob(source_file_name)\n",
    "                blob.download_to_filename(destination_blob_name, timeout=3600)\n",
    "                print(f\"File {source_file_name} download to {destination_blob_name}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: File {source_file_name} download failed\\n{e}\")\n",
    "        \n",
    "    def exist_blob(source_file_name):\n",
    "#         storage_client = storage.Client()\n",
    "#         bucket = storage_client.bucket(bucket_name)\n",
    "        stats = storage.Blob(bucket=bucket, name=source_file_name).exists(storage_client)\n",
    "        if stats:\n",
    "            print(f\"File {source_file_name} exist\")\n",
    "        return stats\n",
    "    \n",
    "    \n",
    "    # %%\n",
    "    def to_float(string):\n",
    "        if string.strip() == '':\n",
    "            return 0\n",
    "        else:\n",
    "            return float(string.strip())\n",
    "\n",
    "    Event = namedtuple(\"event\", [\"time\", \"latitude\", \"longitude\",\n",
    "                                 \"depth_km\", \"magnitude\", \"magnitude_type\", \"index\"])\n",
    "    def read_event_line(line, idx):\n",
    "        time = line[:4]+\"-\"+line[4:6]+\"-\"+line[6:8]+\"T\"+line[8:10]+\":\"+line[10:12]+\":\"+line[12:14]+\".\"+line[14:16] \n",
    "        latitude = to_float(line[16:18]) + to_float(line[19:23])/6000.0\n",
    "        longitude = -(to_float(line[23:26]) + to_float(line[27:31])/6000.0)\n",
    "        if line[18] == 'S':\n",
    "            latitude *= -1.0\n",
    "        if line[26] == 'E':\n",
    "            longitude *= -1.0\n",
    "        depth_km = to_float(line[31:36])/100.0\n",
    "        magnitude_type = (line[146:147]).strip()\n",
    "        magnitude = to_float(line[147:150])/100.0\n",
    "        return Event(time=time, latitude=np.round(latitude, 4), longitude=np.round(longitude,4), \n",
    "                     depth_km=depth_km, magnitude=magnitude, magnitude_type=magnitude_type, index=idx)\n",
    "\n",
    "    def read_p_pick(line):\n",
    "        if float(line[30:32].replace(\" \", \"0\")) < 60:\n",
    "            tp = (line[17:21]+\"-\"+line[21:23]+\"-\"\n",
    "                +line[23:25]+\"T\"+line[25:27]+\":\"\n",
    "                +line[27:29]+\":\"+line[30:32]+'.'\n",
    "                +line[32:34]).replace(\" \", \"0\")\n",
    "            tp = obspy.UTCDateTime(tp)\n",
    "        else:\n",
    "            tp = (line[17:21]+\"-\"+line[21:23]+\"-\"\n",
    "                +line[23:25]+\"T\"+line[25:27]+\":\"\n",
    "                +line[27:29]+\":\"+'00'+'.'\n",
    "                +line[32:34]).replace(\" \", \"0\")\n",
    "            tp = obspy.UTCDateTime(tp) + float(line[30:32].replace(\" \", \"0\"))\n",
    "        remark = line[13:15].strip()\n",
    "        weight = line[16:17].strip()\n",
    "        channel = line[9:12].strip()\n",
    "        return tp, remark, weight, channel\n",
    "\n",
    "    def read_s_pick(line):\n",
    "        if float(line[42:44].replace(\" \", \"0\")) < 60:\n",
    "            ts = (line[17:21]+\"-\"+line[21:23]+\"-\"\n",
    "                +line[23:25]+\"T\"+line[25:27]+\":\"\n",
    "                +line[27:29]+\":\"+line[42:44]+'.'\n",
    "                +line[44:46]).replace(\" \", \"0\")\n",
    "            ts = obspy.UTCDateTime(ts)\n",
    "        else:\n",
    "            ts = (line[17:21]+\"-\"+line[21:23]+\"-\"\n",
    "                +line[23:25]+\"T\"+line[25:27]+\":\"\n",
    "                +line[27:29]+\":\"+\"00\"+'.'\n",
    "                +line[44:46]).replace(\" \", \"0\")\n",
    "            ts = obspy.UTCDateTime(ts) + float(line[42:44].replace(\" \", \"0\"))\n",
    "        remark = line[46:48].strip()\n",
    "        weight = line[49:50].strip()\n",
    "        channel = line[9:12].strip()\n",
    "        return ts, remark, weight, channel\n",
    "\n",
    "    Pick = namedtuple(\"pick\",[\"p_time\", \"p_remark\", \"p_weight\", \"p_channel\",\n",
    "                              \"s_time\", \"s_remark\", \"s_weight\", \"s_channel\",\n",
    "                              \"first_motion\", \"distance_km\",\n",
    "                              \"emergence_angle\", \"azimuth\", \n",
    "                              \"network\", \"station\", \"location_code\", \"event_index\"])\n",
    "    def read_phase_line(p_line, s_line, index):\n",
    "\n",
    "        line = p_line\n",
    "        network = (line[5:7]).strip()\n",
    "        station = (line[:5]).strip()\n",
    "        location_code = (line[111:113]).strip()\n",
    "        distance_km = to_float(line[74:78])/10.0\n",
    "        emergence_angle = to_float(line[78:81])\n",
    "        # duration = to_float(line[87:91])\n",
    "        azimuth = to_float(line[91:94])\n",
    "        first_motion = (line[15:16]).strip()\n",
    "        p_time, p_remark, p_weight, p_channel = read_p_pick(p_line)\n",
    "        s_time, s_remark, s_weight, s_channel = read_s_pick(s_line)\n",
    "\n",
    "        return Pick(p_time=p_time, p_remark=p_remark, p_weight=p_weight, p_channel=p_channel,\n",
    "                    s_time=s_time, s_remark=s_remark, s_weight=s_weight, s_channel=s_channel,\n",
    "                    first_motion=first_motion, distance_km=distance_km, \n",
    "                    emergence_angle=emergence_angle, azimuth=azimuth, \n",
    "                    network=network, station=station, location_code=location_code, event_index=index)\n",
    "\n",
    "    # %%\n",
    "    def resample(stream, default_sampling_rate):\n",
    "        sampling_rate = stream[-1].stats.sampling_rate\n",
    "        if sampling_rate < default_sampling_rate:\n",
    "            print(\"Resample %s\" % stream)\n",
    "            stream.resample(default_sampling_rate) ## resample to 100HZ\n",
    "            print(\"After resample: %s\" % stream)\n",
    "        elif sampling_rate > default_sampling_rate:\n",
    "            print(\"Resample %s\" % stream)\n",
    "            if np.mod(sampling_rate, default_sampling_rate) == 0: ##directly throw away the data\n",
    "                stream.decimate(int(sampling_rate//default_sampling_rate), strict_length=False, no_filter=True)\n",
    "            else:\n",
    "                stream.resample(default_sampling_rate) ## resample to 100HZ\n",
    "            print(\"After resample: %s\" % stream)\n",
    "        return stream\n",
    "\n",
    "    Station = namedtuple(\"station\", [\"id\", \"latitude\", \"longitude\", \"elevation_m\", \"unit\"])\n",
    "    def download_waveform(pick, waveform_path, station_path):\n",
    "\n",
    "        Tstart =  pick.p_time - 60.0\n",
    "        Tend = pick.p_time + 60.0\n",
    "\n",
    "        if pick.p_channel[:-1] != pick.s_channel[:-1]:\n",
    "            channels = [pick.p_channel[:-1], pick.s_channel[:-1]]\n",
    "        else: \n",
    "            channels = [pick.p_channel[:-1]]\n",
    "        channels = set(channels + [\"HN\", \"HH\", \"EH\", \"BH\", \"DP\"])\n",
    "\n",
    "        stream_list = []\n",
    "        station_list = []\n",
    "        for channel in channels:\n",
    "            fname = pick.network+\".\"+pick.station+\".\"+pick.location_code+\".\"+channel+\".\"+f\"{pick.event_index:07d}\"\n",
    "\n",
    "            station_exist = False\n",
    "            station_upload = False\n",
    "            if exist_blob(os.path.join(station_path, fname+\".xml\")):\n",
    "                download_blob(os.path.join(station_path, fname+\".xml\"), os.path.join(station_path, fname+\".xml\"))\n",
    "                station = obspy.read_inventory(os.path.join(station_path, fname+\".xml\"), format=\"STATIONXML\")\n",
    "                os.remove(os.path.join(station_path, fname+\".xml\"))\n",
    "                station_exist = True\n",
    "            else:\n",
    "                try:\n",
    "                    station = client.get_stations(network=pick.network, station=pick.station, location=pick.location_code, \n",
    "                                                  channel=channel+\"?\", starttime=Tstart, endtime=Tend, level=\"response\")\n",
    "                    station_exist = True\n",
    "                    station_upload = True\n",
    "                except Exception as e:\n",
    "                    if str(e)[:len(\"No data available\")] != \"No data available\":\n",
    "                        print(\"Failed downloading station: \"+fname, \"Error: \"+str(e))\n",
    "                        \n",
    "            waveform_exist = False\n",
    "            waveform_upload = False\n",
    "            if exist_blob(os.path.join(waveform_path, fname+\".mseed\")):\n",
    "                download_blob(os.path.join(waveform_path, fname+\".mseed\"), os.path.join(waveform_path, fname+\".mseed\"))\n",
    "                stream = obspy.read(os.path.join(waveform_path, fname+\".mseed\"), format=\"MSEED\")\n",
    "                os.remove(os.path.join(waveform_path, fname+\".mseed\"))\n",
    "                waveform_exist = True\n",
    "            else:\n",
    "                try:                \n",
    "                    stream = client.get_waveforms(network=pick.network, station=pick.station, location=pick.location_code, \n",
    "                                                  channel=channel+\"?\", starttime=Tstart, endtime=Tend)\n",
    "                    stream = stream.detrend('linear')\n",
    "                    stream = stream.merge(fill_value=0)\n",
    "                    stream = stream.trim(Tstart, Tend, pad=True, fill_value=0)\n",
    "                    stream = resample(stream, 100) ## resample to 100 Hz\n",
    "                    stream = stream.sort()\n",
    "                    waveform_exist = True\n",
    "                    waveform_upload = True\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if str(e)[:len(\"No data available\")] != \"No data available\":\n",
    "                        print(\"Failed downloading waveform: \"+fname, \"Error: \"+str(e))\n",
    "            \n",
    "            if waveform_exist and station_exist:\n",
    "                \n",
    "                try:\n",
    "                    stream.attach_response(station)\n",
    "                    stream.remove_sensitivity()\n",
    "                    coord = station.get_coordinates(stream[0].get_id(), datetime=Tstart)\n",
    "                    response = station.get_response(stream[0].get_id(), datetime=Tstart)\n",
    "                except:\n",
    "                    print(\"Error: \", stream[0].get_id(), station)\n",
    "                    continue\n",
    "                    \n",
    "                sta_id = pick.network+\".\"+pick.station+\".\"+pick.location_code+\".\"+channel\n",
    "                unit = response.instrument_sensitivity.input_units.lower()\n",
    "                station_tuple = Station(id=sta_id, latitude=np.round(coord[\"latitude\"], 4), longitude=np.round(coord[\"longitude\"], 4), \n",
    "                                  elevation_m=coord[\"elevation\"], unit=unit)\n",
    "\n",
    "                station_list.append(station_tuple)\n",
    "                stream_list.append(stream)\n",
    "        \n",
    "                if waveform_upload:\n",
    "                    stream.write(os.path.join(waveform_path, fname+\".mseed\"),  format=\"MSEED\")\n",
    "                    upload_blob(os.path.join(waveform_path, fname+\".mseed\"), os.path.join(waveform_path, fname+\".mseed\"))\n",
    "                    os.remove(os.path.join(waveform_path, fname+\".mseed\"))\n",
    "                if station_upload:\n",
    "                    station.write(os.path.join(station_path, fname+\".xml\"), format=\"STATIONXML\")\n",
    "                    upload_blob(os.path.join(station_path, fname+\".xml\"), os.path.join(station_path, fname+\".xml\"))\n",
    "                    os.remove(os.path.join(station_path, fname+\".xml\"))\n",
    "\n",
    "        return stream_list, station_list\n",
    "\n",
    "    def calc_snr(vec, anchor, dt):\n",
    "        npts = int(3/dt)\n",
    "        eps = 10*np.finfo(vec.dtype).eps\n",
    "        snr = (np.std(vec[anchor:anchor+npts, :], axis=0)+eps) / (np.std(vec[anchor-npts:anchor, :], axis=0)+eps)\n",
    "        return snr\n",
    "\n",
    "    def data2vec(i, data, vec, shift, window_size):\n",
    "        if shift >= 0:\n",
    "            if len(data)+shift <= window_size:\n",
    "                vec[shift:len(data)+shift, i] = data\n",
    "            else:\n",
    "                vec[shift:, i] = data[:window_size-shift]\n",
    "\n",
    "        else:\n",
    "            if len(data)+shift <= window_size:\n",
    "                vec[:len(data)+shift, i] = data[-shift:]\n",
    "            else:\n",
    "                vec[:, i] = data[-shift:window_size-shift]\n",
    "        return vec\n",
    "\n",
    "    Extra = namedtuple(\"pick_extra\", [\"p_idx\", \"s_idx\", \"channels\", \"snr\", \"dt\", \"station_index\", \"latitude\", \"longitude\", \"elevation_m\", \"unit\", \"fname\"])\n",
    "    def convert_sample(pick, event, stream, station, sample_path):\n",
    "\n",
    "        dt = stream[-1].stats.delta\n",
    "        npts = stream[-1].stats.npts \n",
    "        starttime = stream[-1].stats.starttime                                  \n",
    "        endtime = stream[-1].stats.endtime\n",
    "\n",
    "        p_idx = int(np.around( (pick.p_time - starttime)/(endtime - starttime)*npts )) \n",
    "        s_idx = int(np.around( (pick.s_time - starttime)/(endtime - starttime)*npts ))\n",
    "\n",
    "        anchor = 6000\n",
    "        window_size = 12000\n",
    "        vec = np.zeros([window_size, 3])\n",
    "        shift = anchor - p_idx\n",
    "        if np.abs(shift) <= 1:\n",
    "            shift = 0\n",
    "        p_idx += shift\n",
    "        s_idx += shift\n",
    "\n",
    "        order = ['3','2','1','E','N','Z']\n",
    "        order = {key: i for i, key in enumerate(order)}\n",
    "        comps = [x.get_id() for x in stream]\n",
    "        try:\n",
    "            comps = sorted(comps, key=lambda x: order[x[-1]])\n",
    "        except:\n",
    "            print(f\"Unknown channels: {comps}\")\n",
    "            return (1, None)\n",
    "\n",
    "        fname = comps[0][:-1]+\".\"+f\"{pick.event_index:07d}\"+\".npz\"\n",
    "        if len(comps) == 3:\n",
    "            for i, c in enumerate(comps):\n",
    "                data = stream.select(id=c)[0].data\n",
    "                data2vec(i, data, vec, shift, window_size)\n",
    "        elif len(comps) < 3:\n",
    "            for c in comps:\n",
    "                if c[-1] == \"E\":\n",
    "                    i = 0\n",
    "                elif c[-1] == \"N\":\n",
    "                    i = 1\n",
    "                elif c[-1] == \"Z\":\n",
    "                    i = 2\n",
    "                else:\n",
    "                    print(f\"Unknown channels: {comps}\")\n",
    "                    return (1, None)\n",
    "                data = stream.select(id=c)[0].data\n",
    "                data2vec(i, data, vec, shift, window_size)\n",
    "        else:\n",
    "            print(f\"Unknown channels: {comps}\")\n",
    "            return (1, None)\n",
    "\n",
    "        snr = calc_snr(vec, anchor, dt)\n",
    "        channels = \",\".join([x.split(\".\")[-1] for x in comps])\n",
    "        extra = Extra(p_idx=p_idx, s_idx=s_idx, snr=tuple(snr.tolist()), dt=dt,\n",
    "                      station_index=station.id, channels=channels,\n",
    "                      latitude=station.latitude, longitude=station.longitude, elevation_m=station.elevation_m,\n",
    "                      unit=station.unit, fname=fname)\n",
    "        np.savez(os.path.join(sample_path, fname), \n",
    "                data=vec.astype(\"float32\"), dt=dt, p_idx=p_idx, s_idx=s_idx, snr=snr.tolist(), \n",
    "                p_time=pick.p_time.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3], s_time=pick.s_time.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3],\n",
    "                p_remark=pick.p_remark, p_weight=pick.p_weight, s_remark=pick.s_remark, s_weight=pick.s_weight,\n",
    "                first_motion=pick.first_motion, distance_km=pick.distance_km, \n",
    "                azimuth=pick.azimuth, emergence_angle=pick.emergence_angle, \n",
    "                network=pick.network, station=pick.station, location_code=pick.location_code, \n",
    "                station_latitude=station.latitude, station_longitude=station.longitude, station_elevation_m=station.elevation_m,\n",
    "                event_latitude=event.latitude, event_longitude=event.longitude, event_depth_km=event.depth_km,\n",
    "                event_time=event.time, event_magnitude=event.magnitude, event_magnitude_type=event.magnitude_type,\n",
    "                unit=station.unit, channels=channels, event_index=pick.event_index)\n",
    "\n",
    "#         upload_blob(os.path.join(sample_path, fname), os.path.join(sample_path, fname))\n",
    "        upload_blob(os.path.join(sample_path, fname), os.path.join(sample_path, fname))\n",
    "        os.remove(os.path.join(sample_path, fname))\n",
    "        return (0, extra)\n",
    "\n",
    "    # %%\n",
    "    def create_dataset(idx, events, phases, waveform_path, sample_path, station_path):\n",
    "        events_ = []\n",
    "        phases_ = []\n",
    "        extras_ = []\n",
    "#         for i in tqdm(idx, desc=\"create dataset\"):\n",
    "        for i in idx:\n",
    "            print(f\"Create dataset {i}/{max(idx)}\")\n",
    "            event = read_event_line(events[i], i)\n",
    "            # events_.append(event)\n",
    "            phase_lines = phases[i]\n",
    "            has_phase = False\n",
    "            for j in range(0, len(phase_lines), 2):\n",
    "                pick = read_phase_line(phase_lines[j], phase_lines[j+1], i)\n",
    "                # phases_.append(pick)\n",
    "                (waveforms, stations) = download_waveform(pick, waveform_path, station_path)\n",
    "                assert(len(waveforms)==len(stations))\n",
    "                for w, s in zip(waveforms, stations):\n",
    "                    (status, extra) = convert_sample(pick, event, w, s, sample_path)\n",
    "                    if (status == 0):\n",
    "                        phases_.append(pick)\n",
    "                        extras_.append(extra)\n",
    "                        has_phase = True\n",
    "            if has_phase:\n",
    "                events_.append(event)\n",
    "\n",
    "        return events_, phases_, extras_\n",
    "\n",
    "    waveform_path = \"mseeds\"\n",
    "    station_path = \"stations\"\n",
    "    sample_path = \"data\"\n",
    "    if not os.path.exists(waveform_path):\n",
    "        os.mkdir(waveform_path)\n",
    "    if not os.path.exists(station_path):\n",
    "        os.mkdir(station_path)\n",
    "    if not os.path.exists(sample_path):\n",
    "        os.mkdir(sample_path)\n",
    "    # events_tuple, phases_tuple, extra_tuple = create_dataset([len(events)-1], events, phases, waveform_path, sample_path)\n",
    "    with open(events_input, \"rb\") as fp:\n",
    "        events = pickle.load(fp)\n",
    "    with open(phases_input, \"rb\") as fp:\n",
    "        phases = pickle.load(fp)\n",
    "        \n",
    "    events_tuple, phases_tuple, extra_tuple = create_dataset(index, events, phases, waveform_path, sample_path, station_path)\n",
    "#     events_tuple, phases_tuple, extra_tuple = create_dataset(range(len(events)//6, len(events)), events, phases, waveform_path, sample_path)\n",
    "\n",
    "    if len(events_tuple) >= 1:\n",
    "        events_df = pd.DataFrame(data=events_tuple)\n",
    "        events_df[\"latitude\"] = events_df[\"latitude\"]\n",
    "        events_df[\"longitude\"] = events_df[\"longitude\"]\n",
    "        events_df = events_df.set_index(\"index\")\n",
    "        events_df.to_csv(f\"{index[0]:03d}_events.csv\", sep=\"\\t\")\n",
    "\n",
    "    if len(phases_tuple) >= 1:\n",
    "        phases_df = pd.DataFrame(data=phases_tuple)\n",
    "        extra_df = pd.DataFrame(data=extra_tuple)\n",
    "        phases_df[\"fname\"] = extra_df[\"fname\"]\n",
    "        phases_df[\"p_idx\"] = extra_df[\"p_idx\"]\n",
    "        phases_df[\"s_idx\"] = extra_df[\"s_idx\"]\n",
    "        phases_df[\"dt\"] = extra_df[\"dt\"]\n",
    "        phases_df[\"latitude\"] = extra_df[\"latitude\"]\n",
    "        phases_df[\"longitude\"] = extra_df[\"longitude\"]\n",
    "        phases_df[\"elevation_m\"] = extra_df[\"elevation_m\"]\n",
    "        phases_df[\"channels\"] = extra_df[\"channels\"]\n",
    "        phases_df[\"unit\"] = extra_df[\"unit\"]\n",
    "        phases_df[\"snr\"] = extra_df[\"snr\"].apply(lambda x: \",\".join([f\"{i:.2f}\" for i in x]))\n",
    "        phases_df[\"p_time\"] = phases_df[\"p_time\"].apply(lambda x: x.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3])\n",
    "        phases_df[\"s_time\"] = phases_df[\"s_time\"].apply(lambda x: x.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3])\n",
    "        phases_df.to_csv(f\"{index[0]:03d}_phases.csv\", sep=\"\\t\", index=False,\n",
    "            columns=[\"fname\", \"network\",\"station\",\"location_code\", \n",
    "            \"p_idx\", \"p_time\",\"p_remark\",\"p_weight\",\n",
    "            \"s_idx\", \"s_time\",\"s_remark\",\"s_weight\",\n",
    "            \"first_motion\",\"distance_km\",\"emergence_angle\",\"azimuth\",\n",
    "            \"latitude\", \"longitude\", \"elevation_m\", \"unit\", \"dt\",\n",
    "            \"event_index\", \"channels\", \"snr\"])\n",
    "        \n",
    "    # %% \n",
    "#     with open(events_output, \"w\") as fout:\n",
    "#         with open(\"event_catalog.csv\") as fin:\n",
    "#             for line in fin:\n",
    "#                 fout.write(line)\n",
    "#     with open(phases_output, \"w\") as fout:\n",
    "#         with open(\"phases.csv\") as fin:\n",
    "#             for line in fin:\n",
    "#                 fout.write(line)\n",
    "    \n",
    "    # %%\n",
    "    if os.path.exists(f\"{index[0]:03d}_events.csv\"):\n",
    "        upload_blob(f\"{index[0]:03d}_events.csv\", os.path.join(\"catalogs\", f\"{index[0]:03d}_events.csv\"))\n",
    "    if os.path.exists(f\"{index[0]:03d}_phases.csv\"):\n",
    "        upload_blob(f\"{index[0]:03d}_phases.csv\", os.path.join(\"catalogs\", f\"{index[0]:03d}_phases.csv\"))\n",
    "    \n",
    "    with open(events_path, \"w\") as fout:\n",
    "        with open(f\"{index[0]:03d}_events.csv\") as fin:\n",
    "            for line in fin:\n",
    "                fout.write(line)\n",
    "    with open(phases_path, \"w\") as fout:\n",
    "        with open(f\"{index[0]:03d}_phases.csv\") as fin:\n",
    "            for line in fin:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brave-shopper",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:11.209332Z",
     "iopub.status.busy": "2021-05-01T05:32:11.209023Z",
     "iopub.status.idle": "2021-05-01T05:32:11.452169Z",
     "shell.execute_reply": "2021-05-01T05:32:11.451364Z",
     "shell.execute_reply.started": "2021-05-01T05:32:11.209307Z"
    }
   },
   "outputs": [],
   "source": [
    "# build_dataset(index = [100000], credentials=credentials, events_input = \"events.pkl\", phases_input = \"phases.pkl\",\n",
    "#               events_output = \"events.out\", phases_output = \"phases.out\")\n",
    "\n",
    "build_dataset_op = comp.func_to_container_op(build_dataset, \n",
    "                                             base_image='python:3.8',\n",
    "                                             packages_to_install= [\n",
    "#                                                   \"tqdm\",\n",
    "                                                  \"obspy\",\n",
    "                                                  \"pandas\",\n",
    "                                                  \"google-cloud-storage\"\n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bored-fundamentals",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:11.463054Z",
     "iopub.status.busy": "2021-05-01T05:32:11.462666Z",
     "iopub.status.idle": "2021-05-01T05:32:11.531151Z",
     "shell.execute_reply": "2021-05-01T05:32:11.530192Z",
     "shell.execute_reply.started": "2021-05-01T05:32:11.463018Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_result(data_path=\"/tmp/\"):\n",
    "    \n",
    "    from glob import glob\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    import re\n",
    "    \n",
    "    # %%\n",
    "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(source_file_name, timeout=3600)\n",
    "        print(f\"File {source_file_name} uploaded to {destination_blob_name}\")\n",
    "        \n",
    "    def download_blob(bucket_name, prefix, data_path, delimiter=\"\"):\n",
    "        if not os.path.exists(data_path):\n",
    "            os.mkdir(data_path)\n",
    "        storage_client = storage.Client()\n",
    "        blobs = storage_client.list_blobs(bucket_name, prefix=prefix, delimiter=delimiter)\n",
    "        for blob in blobs:\n",
    "            if re.match(r\"[0-9]*_(events|phases).csv\", blob.name.split(\"/\")[-1]):\n",
    "                source_blob_name = blob.name\n",
    "                destination_file_name = os.path.join(data_path, blob.name.split(\"/\")[-1])\n",
    "                blob.download_to_filename(destination_file_name)\n",
    "                print(\"Blob {} downloaded to {}.\".format(source_blob_name, destination_file_name))\n",
    "\n",
    "        \n",
    "    download_blob(\"quakeflow\", \"catalogs\", data_path)\n",
    "    join_path = lambda x: os.path.join(data_path, x)\n",
    "    files_events = glob(join_path(\"[0-9]*_events.csv\"))\n",
    "\n",
    "    if len(files_events) > 0:\n",
    "        combined_events = pd.concat([pd.read_csv(f, sep=\"\\t\", dtype=str) for f in files_events ]).sort_values(by=\"time\")\n",
    "        combined_events.to_csv(join_path(\"combined_events.csv\"), sep=\"\\t\", index=False)\n",
    "        upload_blob(\"quakeflow\", join_path(f\"combined_events.csv\"), os.path.join(\"catalogs\", f\"combined_events.csv\"))\n",
    "    else:\n",
    "        print(\"No events.csv found!\")\n",
    "    \n",
    "    files_phases = glob(join_path(\"[0-9]*_phases.csv\"))\n",
    "    if len(files_phases) > 0:\n",
    "        combined_phases = pd.concat([pd.read_csv(f, sep=\"\\t\", dtype=str) for f in files_phases ]).sort_values(by=[\"p_time\"])\n",
    "        combined_phases.to_csv(join_path(\"combined_phases.csv\"), sep=\"\\t\", index=False)\n",
    "        upload_blob(\"quakeflow\", join_path(f\"combined_phases.csv\"), os.path.join(\"catalogs\", f\"combined_phases.csv\"))\n",
    "    else:\n",
    "        print(\"No phases.csv found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prescription-malawi",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:11.533182Z",
     "iopub.status.busy": "2021-05-01T05:32:11.532941Z",
     "iopub.status.idle": "2021-05-01T05:32:11.612641Z",
     "shell.execute_reply": "2021-05-01T05:32:11.611443Z",
     "shell.execute_reply.started": "2021-05-01T05:32:11.533156Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge_result(\"./tmp\")\n",
    "\n",
    "merge_result_op = comp.func_to_container_op(merge_result, \n",
    "                                            base_image='python:3.8',\n",
    "                                            packages_to_install= [\n",
    "                                                  \"pandas\",\n",
    "                                                  \"google-cloud-storage\",\n",
    "                                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "genuine-noise",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:11.614678Z",
     "iopub.status.busy": "2021-05-01T05:32:11.614351Z",
     "iopub.status.idle": "2021-05-01T05:32:11.664250Z",
     "shell.execute_reply": "2021-05-01T05:32:11.663374Z",
     "shell.execute_reply.started": "2021-05-01T05:32:11.614649Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_hdf5(data_path=\"/tmp/\"):\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    import h5py\n",
    "#     from tqdm import tqdm\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "#     data_path = \"/tmp/npz\"\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    bucket_name = \"quakeflow\"\n",
    "    prefix = \"data\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    blob = bucket.blob(\"catalogs/combined_events.csv\")\n",
    "    blob.download_to_filename(\"combined_events.csv\", timeout=3600)\n",
    "    print(f\"File catalogs/combined_events.csv download to combined_events.csv\")\n",
    "    blob = bucket.blob(\"catalogs/combined_phases.csv\")\n",
    "    blob.download_to_filename(\"combined_phases.csv\", timeout=3600)\n",
    "    print(f\"File catalogs/combined_phases.csv download to combined_phases.csv\")            \n",
    "\n",
    "    file_name = \"ncedc.h5\"\n",
    "\n",
    "    events = pd.read_csv(\"combined_events.csv\", sep=\"\\t\").sort_values(\"index\")\n",
    "    events.to_hdf(file_name, '/events', format=\"table\", mode='w')\n",
    "    print(events)\n",
    "    catalogs = pd.read_csv(\"combined_phases.csv\", sep=\"\\t\")\n",
    "    catalogs.to_hdf(file_name, '/catalog', format=\"table\", mode='r+')\n",
    "    print(catalogs)\n",
    "\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix, delimiter=\"\")\n",
    "    # blobs = list(blobs)\n",
    "    with h5py.File(file_name, \"r+\", libver='latest') as fp:\n",
    "        data = fp.create_group(\"/data\")\n",
    "    #     for blob in tqdm(blobs):\n",
    "#         for blob in blobs:\n",
    "#         for fname in tqdm(catalogs[\"fname\"]):\n",
    "        for fname in catalogs[\"fname\"]:\n",
    "#             source_blob_name = blob.name\n",
    "#             fname = blob.name.split(\"/\")[-1]\n",
    "            source_blob_name = \"data/{fname}\"\n",
    "            destination_file_name = os.path.join(data_path, fname)\n",
    "            blob.download_to_filename(destination_file_name)\n",
    "            print(\"Blob {} downloaded to {}.\".format(source_blob_name, destination_file_name))\n",
    "\n",
    "            meta = np.load(destination_file_name)\n",
    "            ds = data.create_dataset(fname, data=meta[\"data\"], dtype=\"float32\")\n",
    "            for k in meta:\n",
    "                if k != \"data\":\n",
    "                    if meta[k].dtype.type is np.str_:\n",
    "                        ds.attrs[k] = str(meta[k])\n",
    "                    else:\n",
    "                        ds.attrs[k] = meta[k]\n",
    "    #         print(ds.shape, dict(ds.attrs))\n",
    "\n",
    "            os.system(f\"rm {destination_file_name}\")\n",
    "#             print(f\"rm {destination_file_name}\")\n",
    "    #         raise\n",
    "#             break\n",
    "\n",
    "    \n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.upload_from_filename(file_name, timeout=3600)\n",
    "    print(f\"File {file_name} uploaded to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "civilian-ozone",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:11.665859Z",
     "iopub.status.busy": "2021-05-01T05:32:11.665530Z",
     "iopub.status.idle": "2021-05-01T05:32:11.739306Z",
     "shell.execute_reply": "2021-05-01T05:32:11.738546Z",
     "shell.execute_reply.started": "2021-05-01T05:32:11.665833Z"
    }
   },
   "outputs": [],
   "source": [
    "merge_hdf5_op = comp.func_to_container_op(merge_hdf5, \n",
    "                                        base_image='python:3.8',\n",
    "                                        packages_to_install= [\n",
    "                                              \"pandas\",\n",
    "                                              \"google-cloud-storage\",\n",
    "                                              \"numpy\",\n",
    "                                              \"h5py\",\n",
    "                                              \"tables\"\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "secondary-article",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:11.741288Z",
     "iopub.status.busy": "2021-05-01T05:32:11.740995Z",
     "iopub.status.idle": "2021-05-01T05:32:11.791873Z",
     "shell.execute_reply": "2021-05-01T05:32:11.790585Z",
     "shell.execute_reply.started": "2021-05-01T05:32:11.741262Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(name='QuakeFlow', description='')\n",
    "def dataset_pipeline():\n",
    "    \n",
    "    data_path = \"/tmp/\"\n",
    "    raw_catalog = download_catalog_op()#.set_memory_request(\"60G\")\n",
    "    raw_catalog.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "    \n",
    "    ps_catalog = read_ps_catalog_op(raw_catalog.outputs[\"catalog\"])\n",
    "    ps_catalog.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "    \n",
    "    with kfp.dsl.ParallelFor(ps_catalog.outputs[\"output\"]) as idx:\n",
    "        \n",
    "        build_dataset_op_ = build_dataset_op(idx, ps_catalog.outputs[\"index\"], ps_catalog.outputs[\"events\"], ps_catalog.outputs[\"phases\"]).set_memory_request(\"1100M\")#.set_retry(1)\n",
    "        build_dataset_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "\n",
    "    csv = merge_result_op(data_path).after(build_dataset_op_)\n",
    "    csv.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "#     hdf5 = merge_hdf5_op(data_path).after(csv)\n",
    "#     hdf5.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "external-section",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-01T05:32:11.793166Z",
     "iopub.status.busy": "2021-05-01T05:32:11.792885Z",
     "iopub.status.idle": "2021-05-01T05:32:13.023563Z",
     "shell.execute_reply": "2021-05-01T05:32:13.022715Z",
     "shell.execute_reply.started": "2021-05-01T05:32:11.793135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://10b36473d207cad7-dot-us-west1.pipelines.googleusercontent.com/#/experiments/details/e07e974c-9265-47ab-aef2-5296c076e8dd\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://10b36473d207cad7-dot-us-west1.pipelines.googleusercontent.com/#/runs/details/bbd4fae2-9381-4b79-909d-67c6f5a7a82e\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/weiqiang/.dotbot/cloud/quakeflow_wayne.json\"\n",
    "\n",
    "host_url = \"10b36473d207cad7-dot-us-west1.pipelines.googleusercontent.com\"\n",
    "client = kfp.Client(host=host_url)\n",
    "\n",
    "experiment_name = 'Dataset'\n",
    "pipeline_func = dataset_pipeline\n",
    "run_name = pipeline_func.__name__ + '_run'\n",
    "\n",
    "arguments = {}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "# kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "results = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                               experiment_name=experiment_name, \n",
    "                                               run_name=run_name, \n",
    "                                               arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-sussex",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-clone",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
